from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader,PyPDFLoader,WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pathlib import Path
import shutil
import warnings
import logging

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

load_dotenv()

# Configurer le logging pour r√©duire le bruit
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Supprimer les avertissements non critiques de PyPDF
warnings.filterwarnings('ignore', category=UserWarning, module='pypdf')
warnings.filterwarnings('ignore', message='.*Multiple definitions in dictionary.*')

# Resolve data paths relative to the project root (one level up from `src/`)
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_PATH = BASE_DIR / "data"
DATA_DB_PATH = BASE_DIR / "data" / "chroma_db"
URL_CONSTITUTION = "https://conseilconstitutionnel.sn/la-constitution/"
URL_CODE_DES_COLLECTIVITES_LOCALES ="https://primature.sn/publications/lois-et-reglements/code-des-collectivites-locales"
URL_CODE_DE_AVIATION_CIVILE="https://primature.sn/publications/lois-et-reglements/code-de-laviation-civile"
URL_MISE_A_JOUR_CONSTITUTION="https://primature.sn/publications/lois-et-reglements/mises-jour-de-la-constitution"

def ingest_documents():
    """Ing√®re les documents PDF et web, les d√©coupe en chunks et les stocke dans Chroma."""
    logger.info(f"üìö D√©but de l'ingestion des documents depuis : {DATA_PATH}")

    if not DATA_PATH.exists():
        raise FileNotFoundError(
            f"Data directory not found: '{DATA_PATH}'.\nExpected 'data' directory at project root ({BASE_DIR})."
        )

    # 1. Chargement des documents PDF
    logger.info("üìÑ Chargement des documents PDF...")
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            loader_pdf = DirectoryLoader(
                path=str(DATA_PATH),
                glob="**/*.pdf",
                loader_cls=PyPDFLoader,
                show_progress=True,
            )
            documents_pdf = loader_pdf.load()
        logger.info(f"‚úÖ {len(documents_pdf)} documents PDF charg√©s avec succ√®s.")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement des PDFs: {e}")
        documents_pdf = []
    
    # 2. Chargement des documents web
    logger.info("üåê Chargement des documents web...")
    try:
        loader_web = WebBaseLoader(
            web_path=[
                URL_CONSTITUTION,
                URL_CODE_DES_COLLECTIVITES_LOCALES,
                URL_CODE_DE_AVIATION_CIVILE,
                URL_MISE_A_JOUR_CONSTITUTION
            ],
            show_progress=True,
        )
        documents_web = loader_web.load()
        logger.info(f"‚úÖ {len(documents_web)} documents web charg√©s avec succ√®s.")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement des documents web: {e}")
        documents_web = []
    
    documents = documents_pdf + documents_web
    logger.info(f"üì¶ Total: {len(documents)} documents charg√©s ({len(documents_pdf)} PDF + {len(documents_web)} web).")
    
    # 3. D√©coupage (chunking) des documents
    logger.info("‚úÇÔ∏è  D√©coupage des documents en chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
    )
    chunks = text_splitter.split_documents(documents)
    logger.info(f"‚úÖ {len(chunks)} chunks cr√©√©s apr√®s d√©coupage.")
    
    # 4. Pr√©paration de la base de donn√©es
    if DATA_DB_PATH.exists():
        logger.warning(f"‚ö†Ô∏è  Suppression de l'ancienne base de donn√©es √† : {DATA_DB_PATH}")
        shutil.rmtree(DATA_DB_PATH)
    
    # Cr√©er le r√©pertoire s'il n'existe pas
    DATA_DB_PATH.mkdir(parents=True, exist_ok=True)
    
    # 5. V√©rification et filtrage des chunks valides
    logger.info("üîç V√©rification de la validit√© des chunks...")
    valid_chunks = [
        chunk for chunk in chunks 
        if chunk.page_content and len(chunk.page_content.strip()) > 0
    ]
    invalid_count = len(chunks) - len(valid_chunks)
    if invalid_count > 0:
        logger.warning(f"‚ö†Ô∏è  {invalid_count} chunks invalides ignor√©s (vides ou sans contenu).")
    logger.info(f"üìä {len(valid_chunks)} chunks valides sur {len(chunks)} total.")
    
    if len(valid_chunks) == 0:
        logger.error("‚ùå ERREUR: Aucun chunk valide √† stocker!")
        return
    
    # 6. Cr√©ation des embeddings et stockage dans Chroma
    logger.info("üîÑ Initialisation du mod√®le d'embeddings...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    logger.info("üîÑ Cr√©ation de la base de donn√©es Chroma...")
    logger.info(f"   üìÅ R√©pertoire: {DATA_DB_PATH}")
    logger.info(f"   üì¶ Nombre de documents: {len(valid_chunks)}")
    logger.info("   ‚è≥ Cela peut prendre plusieurs minutes...")
    
    # Cr√©er la base avec from_documents (cela peut prendre du temps)
    try:
        Chroma.from_documents(
            documents=valid_chunks,
            embedding=embedding_model,
            persist_directory=str(DATA_DB_PATH),
            collection_name="juridiction_senegal",
        )
        logger.info("‚úÖ Base de donn√©es Chroma cr√©√©e avec succ√®s.")
    except (ValueError, RuntimeError, OSError) as e:
        logger.error(f"‚ùå ERREUR lors de la cr√©ation: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 7. V√©rification de la persistance
    logger.info("üîÑ V√©rification de la persistance...")
    
    try:
        import time
        logger.info("   ‚è≥ Attente de 2 secondes pour la persistance...")
        time.sleep(2)
        
        # V√©rifier les fichiers cr√©√©s
        if DATA_DB_PATH.exists():
            files = list(DATA_DB_PATH.iterdir())
            logger.info(f"   üìÇ Fichiers cr√©√©s: {len(files)}")
            total_size = 0
            for f in files[:10]:
                if f.is_file():
                    size = f.stat().st_size
                    total_size += size
                    logger.info(f"      - {f.name} ({size:,} bytes)")
            logger.info(f"   üíæ Taille totale: {total_size:,} bytes ({total_size / 1024 / 1024:.2f} MB)")
        
        # Recharger la base pour v√©rifier
        logger.info("üîç Rechargement de la base de donn√©es pour v√©rification...")
        db_check = Chroma(
            persist_directory=str(DATA_DB_PATH),
            embedding_function=embedding_model,
            collection_name="juridiction_senegal"
        )
        
        # V√©rifier le nombre de documents
        # Note: Acc√®s √† _collection n√©cessaire pour v√©rifier le count
        collection = db_check._collection  # type: ignore[attr-defined]
        count = collection.count() if collection else 0
        logger.info(f"‚úÖ Base de donn√©es Chroma cr√©√©e √† : {DATA_DB_PATH}")
        logger.info(f"‚úÖ {count} documents stock√©s dans la base de donn√©es.")
        
        if count == 0:
            logger.error("‚ùå ERREUR: Aucun document n'a √©t√© stock√©!")
            logger.info("üí° Tentative de diagnostic...")
            
            if DATA_DB_PATH.exists():
                files = list(DATA_DB_PATH.iterdir())
                logger.info(f"   üìÇ Fichiers dans le r√©pertoire: {len(files)}")
                for f in files[:5]:
                    logger.info(f"      - {f.name}")
            
            try:
                import chromadb
                client = chromadb.PersistentClient(path=str(DATA_DB_PATH))
                collections = client.list_collections()
                logger.info(f"   üìö Collections trouv√©es: {len(collections)}")
                for col in collections:
                    col_count = col.count()
                    logger.info(f"      - {col.name}: {col_count} documents")
                    if col_count > 0:
                        logger.info(f"         ‚úÖ Collection '{col.name}' contient {col_count} documents!")
            except (ImportError, RuntimeError, OSError) as e2:
                logger.error(f"   ‚ö†Ô∏è  Erreur lors de la v√©rification des collections: {e2}")
                import traceback
                traceback.print_exc()
            
            return
        
        # Test de r√©cup√©ration pour confirmer
        logger.info("üîç Test de r√©cup√©ration...")
        test_results = db_check.similarity_search("test", k=1)
        if test_results:
            logger.info(f"‚úÖ Test de r√©cup√©ration r√©ussi: {len(test_results)} document(s) trouv√©(s)")
            logger.info(f"   üìÑ Aper√ßu: {test_results[0].page_content[:100]}...")
        else:
            logger.warning("‚ö†Ô∏è  Avertissement: Test de r√©cup√©ration n'a retourn√© aucun r√©sultat")
            
    except (ValueError, RuntimeError, OSError, ImportError) as e:
        logger.error(f"‚ö†Ô∏è  Erreur lors de la v√©rification : {e}")
        import traceback
        traceback.print_exc()
        logger.info(f"‚úÖ Base de donn√©es Chroma cr√©√©e √† : {DATA_DB_PATH}")
    
if __name__ == "__main__":
    logger.info(f"üìÅ R√©pertoire de base: {BASE_DIR}")
    try:
        ingest_documents()
        logger.info("üéâ Ingestion termin√©e avec succ√®s!")
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale lors de l'ingestion: {e}")
        import traceback
        traceback.print_exc()
        exit(1)  