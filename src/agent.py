from dotenv import load_dotenv
from typing import List, TypedDict, Optional
import os

from langchain_groq import ChatGroq
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# Custom BGE Reranker implementation
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import gc


class BGEReranker:
    """Custom reranker using BGE reranker model from HuggingFace with lazy loading."""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-base", top_n: int = 3, device: Optional[str] = None):
        """
        Initialize the BGE reranker with lazy loading.
        
        Args:
            model_name: Name of the BGE reranker model from HuggingFace
            top_n: Number of top documents to return after reranking
            device: Device to run the model on ('cuda', 'cpu', or None for auto)
        """
        self.model_name = model_name
        self.top_n = top_n
        self._tokenizer = None
        self._model = None
        
        # Auto-detect device if not specified
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
    
    @property
    def tokenizer(self):
        """Lazy load tokenizer only when needed."""
        if self._tokenizer is None:
            # Use use_fast=False to avoid loading the large tokenizer.json if possible
            # Try to load with memory-efficient settings
            try:
                self._tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    use_fast=False,  # Use slower but more memory-efficient tokenizer
                    local_files_only=False
                )
            except Exception as e:
                print(f"Error loading tokenizer: {e}")
                # Fallback: try with use_fast=True
                self._tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    use_fast=True,
                    local_files_only=False
                )
        return self._tokenizer
    
    @property
    def model(self):
        """Lazy load model only when needed."""
        if self._model is None:
            try:
                # Load model with low_cpu_mem_usage if available
                self._model = AutoModelForSequenceClassification.from_pretrained(
                    self.model_name,
                    low_cpu_mem_usage=True,
                    torch_dtype=torch.float32 if self.device == "cpu" else torch.float16
                )
                self._model.to(self.device)
                self._model.eval()
            except Exception as e:
                print(f"Error loading reranker model: {e}")
                raise
        return self._model
    
    def compress_documents(
        self, documents: List[Document], query: str, batch_size: int = 8
    ) -> List[Document]:
        """
        Rerank documents based on relevance to the query.
        Processes documents in batches to reduce memory usage.
        
        Args:
            documents: List of Document objects to rerank
            query: Query string to rank documents against
            batch_size: Number of documents to process at once (default: 8)
            
        Returns:
            List of reranked Document objects (top_n)
        """
        if not documents:
            return documents
        
        # If we have fewer documents than top_n, just return all
        if len(documents) <= self.top_n:
            return documents
        
        scored_docs = []
        
        # Process in batches to reduce memory usage
        try:
            with torch.no_grad():
                for i in range(0, len(documents), batch_size):
                    batch_docs = documents[i:i + batch_size]
                    pairs = [[query, doc.page_content] for doc in batch_docs]
                    
                    # Tokenize batch
                    inputs = self.tokenizer(
                        pairs,
                        padding=True,
                        truncation=True,
                        return_tensors="pt",
                        max_length=512
                    ).to(self.device)
                    
                    # Get scores for batch
                    scores = self.model(**inputs, return_dict=True).logits.view(-1).float()
                    
                    # Store scores with documents
                    batch_scores = scores.cpu().tolist()
                    scored_docs.extend(list(zip(batch_scores, batch_docs)))
                    
                    # Clear cache periodically
                    if self.device == "cuda":
                        torch.cuda.empty_cache()
                    
                    # Force garbage collection every few batches
                    if i % (batch_size * 4) == 0:
                        gc.collect()
            
            # Sort by score (descending) and take top_n
            scored_docs.sort(key=lambda x: x[0], reverse=True)
            top_docs = [doc for _, doc in scored_docs[:self.top_n]]
            
            return top_docs
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower() or "memory" in str(e).lower():
                print(f"Memory error during reranking: {e}")
                print("Falling back to returning original documents without reranking.")
                # Return original documents if reranking fails due to memory
                return documents[:self.top_n]
            else:
                raise


load_dotenv()

# Utiliser un chemin absolu pour √©viter les probl√®mes de chemin relatif
from pathlib import Path
import os

BASE_DIR = Path(__file__).resolve().parent.parent
CHROMA_DB_PATH = BASE_DIR / "data" / "chroma_db"

# Configuration depuis les variables d'environnement
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEY n'est pas d√©finie dans les variables d'environnement")

# Initialiser l'embedding function (lazy loading pour optimiser le d√©marrage)
_embedding_function = None
_db = None
_retriever = None

def get_embedding_function():
    """Lazy loading de l'embedding function."""
    global _embedding_function
    if _embedding_function is None:
        _embedding_function = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},  # Utiliser CPU par d√©faut pour √©viter les probl√®mes GPU
            encode_kwargs={'normalize_embeddings': True}  # Normaliser pour de meilleures performances
        )
    return _embedding_function

def get_db():
    """Lazy loading de la base de donn√©es Chroma."""
    global _db
    if _db is None:
        if not CHROMA_DB_PATH.exists():
            raise FileNotFoundError(
                f"Base de donn√©es Chroma introuvable: {CHROMA_DB_PATH}\n"
                "Ex√©cutez: python src/ingestion.py"
            )
        
        _db = Chroma(
            persist_directory=str(CHROMA_DB_PATH),
            embedding_function=get_embedding_function(),
            collection_name="juridiction_senegal"
        )
        
        # V√©rifier que la base de donn√©es contient des documents
        try:
            collection = _db._collection
            count = collection.count() if collection else 0
            if count == 0:
                print("‚ö†Ô∏è  ATTENTION: La base de donn√©es Chroma est vide! Ex√©cutez: python src/ingestion.py")
            else:
                print(f"‚úÖ Base de donn√©es Chroma charg√©e: {count} documents disponibles")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la v√©rification de la base de donn√©es: {e}")
    
    return _db

def get_retriever():
    """Lazy loading du retriever."""
    global _retriever
    if _retriever is None:
        db = get_db()
        _retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # Limiter √† 5 documents pour de meilleures performances
        )
    return _retriever

# Initialiser au d√©marrage (peut √™tre comment√© pour un vrai lazy loading)
try:
    db = get_db()
    retriever = get_retriever()
except Exception as e:
    print(f"‚ö†Ô∏è  Erreur lors de l'initialisation de la base de donn√©es: {e}")
    db = None
    retriever = None

# Configuration des LLMs avec gestion d'erreur
try:
    router_llm = ChatGroq(
        model_name="llama-3.1-8b-instant",
        temperature=0,
        max_tokens=100,  # Limiter pour le routage
        timeout=30,  # Timeout de 30 secondes
    )
    generation_llm = ChatGroq(
        model_name="llama-3.1-8b-instant",
        temperature=0,
        max_tokens=2000,  # Limiter la longueur des r√©ponses
        timeout=60,  # Timeout de 60 secondes
    )
except Exception as e:
    print(f"‚ùå Erreur lors de l'initialisation des LLMs: {e}")
    raise

# Initialiser le checkpointer pour la m√©moire des conversations
memory = MemorySaver()

class AgentState(TypedDict):
    question: str
    documents: List[Document]
    category: str
    answer: str
    sources: List[str]
    messages: List  # Historique des messages pour le checkpointer
    
    
    
def classify_question(state: AgentState):
    """N≈ìud qui utilise le router_llm pour classer la question (le plus rapide)."""
    # Ajouter le message de l'utilisateur √† l'historique
    messages = state.get("messages", [])
    messages.append(HumanMessage(content=state["question"]))
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Tu es un classificateur binaire. Ta seule t√¢che est de d√©terminer si la question de l'utilisateur concerne le droit s√©n√©galais (Constitution, Code du Travail, Code P√©nal, etc.). R√©ponds UNIQUEMENT avec le mot 'JURIDIQUE' si oui, ou 'AUTRE' si non."),
        ("human", "{question}")
    ])
    
    chain = prompt | router_llm # FIX : Utilise SEULEMENT le router_llm
    category = chain.invoke({"question": state["question"]}).content.strip().upper()
    
    return {"category": category, "messages": messages}

def route_question(state: AgentState):
    if state.get("category") == "JURIDIQUE":
        return "retrieve"
    return "refuse" # <-- Chang√© de "end" √† "refuse"

def handle_non_juridique(state: AgentState):
    """G√©n√®re une r√©ponse polie avec le router_llm lorsque la question est hors-sujet."""
    question = state["question"]
    messages = state.get("messages", [])
    
    template = "La question suivante n'est pas juridique: '{question}'. R√©dige une r√©ponse polie et concise (moins de 20 mots) indiquant que tu ne peux pas r√©pondre car tu es un assistant juridique sp√©cialis√© dans le droit s√©n√©galais."
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | router_llm # FIX : Utilise SEULEMENT le router_llm (pour la vitesse)
    
    response = chain.invoke({"question": question})
    
    # Ajouter la r√©ponse √† l'historique
    messages.append(AIMessage(content=response.content))
    
    return {
        "answer": response.content,
        # CHANGEMENT : on utilise la cl√© 'sources'
        "sources": ["Question jug√©e hors du champ d'expertise juridique."],
        "messages": messages,
        "suggested_questions": []
    }

def detect_domain_from_source(source_path: str) -> str:
    """D√©tecte le domaine juridique √† partir du chemin de la source."""
    source_lower = source_path.lower()
    
    # D√©tection bas√©e sur les chemins de fichiers et URLs
    if 'droitsocial' in source_lower or 'codedutravail' in source_lower or 'travail' in source_lower or 'retraite' in source_lower:
        return 'travail'
    elif 'droitpenal' in source_lower or 'penal' in source_lower or 'prescription' in source_lower:
        return 'penal'
    elif 'finance' in source_lower or 'budget' in source_lower or 'loi de finances' in source_lower:
        return 'finance'
    elif 'organisationadministration' in source_lower or 'fonction publique' in source_lower or 'administration' in source_lower:
        return 'administration'
    elif 'constitution' in source_lower or 'conseilconstitutionnel' in source_lower:
        return 'constitution'
    elif 'collectivites' in source_lower or 'collectivit√©s' in source_lower:
        return 'collectivites'
    elif 'aviation' in source_lower:
        return 'aviation'
    else:
        return 'general'

def generate_suggested_questions(question: str, documents: List[Document], answer: str) -> List[str]:
    """G√©n√®re des questions sugg√©r√©es bas√©es sur les documents r√©els et leur domaine."""
    if not documents:
        return []
    
    # D√©tecter les domaines des documents retourn√©s
    domains = set()
    for doc in documents:
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        source = metadata.get('source', metadata.get('file_path', ''))
        domain = detect_domain_from_source(str(source))
        domains.add(domain)
    
    # D√©terminer le domaine principal (le plus fr√©quent ou le premier)
    primary_domain = list(domains)[0] if domains else 'general'
    
    # Extraire des mots-cl√©s de la question et de la r√©ponse
    question_lower = question.lower()
    answer_lower = answer.lower()
    
    # Questions sugg√©r√©es par domaine bas√©es sur les documents r√©els
    domain_questions = {
        'travail': [
            'Quelle est la dur√©e l√©gale du pr√©avis de d√©mission ?',
            'Comment sont calcul√©s les cong√©s pay√©s ?',
            'Quels sont mes droits en cas de licenciement abusif ?',
            'Quelles sont les conditions d\'un CDD ?',
            'Quelle est la dur√©e maximale du temps de travail ?',
            'Comment fonctionne la retraite au S√©n√©gal ?',
            'Quels sont les droits des travailleurs en cas de gr√®ve ?',
        ],
        'penal': [
            'Quelles sont les peines encourues pour cette infraction ?',
            'Quel est le d√©lai de prescription en droit p√©nal ?',
            'Comment se d√©roule la proc√©dure p√©nale ?',
            'Quels sont les recours possibles en droit p√©nal ?',
            'Quelles sont les conditions de suspension des d√©lais de prescription ?',
        ],
        'finance': [
            'Quels sont les principes budg√©taires de l\'√âtat ?',
            'Comment est organis√©e la nomenclature budg√©taire ?',
            'Quelles sont les voies et moyens de l\'√âtat ?',
            'Comment fonctionne le budget vert ?',
            'Quelle est la strat√©gie de gestion de la dette ?',
        ],
        'administration': [
            'Quelle est l\'organisation de la fonction publique ?',
            'Comment sont structur√©es les administrations centrales ?',
            'Quels sont les pouvoirs du Pr√©sident de la R√©publique ?',
            'Comment fonctionne la r√©partition des services de l\'√âtat ?',
        ],
        'constitution': [
            'Quels sont les droits fondamentaux garantis par la Constitution ?',
            'Comment est organis√©e la s√©paration des pouvoirs ?',
            'Quels sont les pouvoirs du Pr√©sident de la R√©publique ?',
            'Comment fonctionne le Parlement ?',
            'Quelle est la proc√©dure de r√©vision constitutionnelle ?',
        ],
        'collectivites': [
            'Quels sont les pouvoirs des collectivit√©s territoriales ?',
            'Quelle est l\'organisation territoriale du S√©n√©gal ?',
            'Quels sont les budgets des collectivit√©s locales ?',
            'Comment fonctionnent les √©lections locales ?',
        ],
        'aviation': [
            'Quelles sont les r√®gles de s√©curit√© a√©rienne ?',
            'Quels sont les droits des passagers a√©riens ?',
            'Quelle est la responsabilit√© du transporteur a√©rien ?',
            'Quels sont les documents requis pour un vol international ?',
        ],
        'general': [
            'Quels sont les d√©lais applicables ?',
            'Quelle est la proc√©dure √† suivre ?',
            'Quels sont les recours possibles ?',
            'Y a-t-il des exceptions pr√©vues ?',
        ]
    }
    
    # Obtenir les questions du domaine principal
    available_questions = domain_questions.get(primary_domain, domain_questions['general'])
    
    # Filtrer les questions qui sont trop similaires √† la question actuelle
    filtered_questions = []
    for q in available_questions:
        # √âviter les questions trop similaires
        q_lower = q.lower()
        similarity = sum(1 for word in question_lower.split() if word in q_lower and len(word) > 3)
        if similarity < 3:  # Moins de 3 mots en commun
            filtered_questions.append(q)
    
    # Si on n'a pas assez de questions, compl√©ter avec des questions g√©n√©rales
    while len(filtered_questions) < 3 and len(available_questions) > len(filtered_questions):
        for q in available_questions:
            if q not in filtered_questions:
                filtered_questions.append(q)
                break
    
    # Prendre 3 questions au maximum
    return filtered_questions[:3]

def retrieve_noeud(state: AgentState):

    question = state["question"]
    # Use the Chroma retriever to fetch relevant documents for the question
    try:
        documents = retriever.invoke(question)
        return {"documents": documents}
    except Exception as e:
        print(f"‚ùå ERREUR dans retrieve_noeud: {e}")
        return {"documents": []}

def generate_node(state: AgentState):
    """G√©n√®re la r√©ponse finale en utilisant le mod√®le de g√©n√©ration."""
    question = state["question"]
    documents = state.get("documents", [])
    messages = state.get("messages", [])
    
    # Construire l'historique de conversation √† partir des messages pr√©c√©dents
    history_str = ""
    if len(messages) > 1:  # Plus qu'un seul message (la question actuelle)
        # Prendre les 5 derniers √©changes (10 messages max)
        recent_messages = messages[-10:]
        history_parts = []
        for msg in recent_messages:
            if isinstance(msg, HumanMessage):
                history_parts.append(f"Utilisateur: {msg.content}")
            elif isinstance(msg, AIMessage):
                history_parts.append(f"Assistant: {msg.content}")
        if history_parts:
            history_str = "\n".join(history_parts)
    
    # Pr√©parer le contexte (sans r√©f√©rences inline) et les sources pour l'affichage
    context_parts = []
    sources_list = []
    
    if not documents:
        return {
            "answer": "Je ne trouve pas l'information dans les textes fournis.",
            "sources": ["Aucun document trouv√© pour cette question."],
            "messages": messages,
            "suggested_questions": []
        }
    
    # Extraire les mots-cl√©s de la question pour trouver les parties pertinentes
    question_words = set(question.lower().split())
    
    for doc in documents:
        # Extraire les m√©tadonn√©es
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        source = metadata.get('source', metadata.get('file_path', 'Document juridique'))
        page = metadata.get('page', metadata.get('page_number', ''))
        
        # Nettoyer le nom de la source (enlever le chemin complet si pr√©sent)
        if isinstance(source, str):
            source_name = os.path.basename(source) if os.path.sep in source else source
            # Enlever l'extension si pr√©sente
            source_name = os.path.splitext(source_name)[0]
        else:
            source_name = str(source)
        
        # Ajouter le contenu au contexte (sans r√©f√©rence inline)
        if doc.page_content:
            context_parts.append(doc.page_content)
        
        # Formater la source pour l'affichage dans la section "üìö Voir les articles de loi sources"
        if page:
            source_info = f"{source_name} (page {page})"
        else:
            source_info = f"{source_name}"
        
        # Extraire un extrait pertinent du contenu
        content = doc.page_content if doc.page_content else "(Contenu vide)"
        
        # Si le contenu est long, essayer de trouver la partie la plus pertinente
        if len(content) > 500:
            # Chercher les phrases contenant des mots-cl√©s de la question
            sentences = content.split('.')
            relevant_sentences = []
            
            for sentence in sentences:
                sentence_lower = sentence.lower()
                # Compter combien de mots-cl√©s sont pr√©sents dans la phrase
                matches = sum(1 for word in question_words if word in sentence_lower and len(word) > 3)
                if matches > 0:
                    relevant_sentences.append((matches, sentence))
            
            if relevant_sentences:
                # Trier par pertinence et prendre les meilleures phrases
                relevant_sentences.sort(reverse=True, key=lambda x: x[0])
                # Prendre jusqu'√† 3-4 phrases les plus pertinentes
                selected_sentences = [s[1] for s in relevant_sentences[:4]]
                # Trouver leur position dans le texte original
                start_pos = content.find(selected_sentences[0])
                if start_pos > 0:
                    # Commencer un peu avant pour avoir du contexte
                    start_pos = max(0, start_pos - 100)
                end_pos = content.find(selected_sentences[-1]) + len(selected_sentences[-1])
                if end_pos < len(content):
                    # Finir un peu apr√®s pour avoir du contexte
                    end_pos = min(len(content), end_pos + 100)
                
                # Extraire l'extrait pertinent
                extracted_content = content[start_pos:end_pos].strip()
                # Ajouter "..." si n√©cessaire
                if start_pos > 0:
                    extracted_content = "..." + extracted_content
                if end_pos < len(content):
                    extracted_content = extracted_content + "..."
                
                content = extracted_content
            else:
                # Si pas de correspondance, prendre le d√©but (mais essayer de commencer par une phrase compl√®te)
                first_period = content.find('.')
                if first_period > 0 and first_period < 200:
                    content = content[:min(600, len(content))]
                else:
                    # Prendre les premiers 500 caract√®res
                    content = content[:500] + "..."
        
        # Stocker la source avec le contenu format√©
        source_text = f"{source_info}\n\n{content}"
        sources_list.append(source_text)
    
    context = "\n\n".join(context_parts)
    
    # Construire le template avec l'historique si disponible
    if history_str:
        template = """TU ES UN ASSISTANT JURIDIQUE S√âN√âGALAIS STRICTEMENT FACTUEL. 
    TON R√îLE UNIQUE est de r√©pondre aux questions de l'utilisateur en te basant EXCLUSIVEMENT sur les extraits de loi CONTEXTE. 
    Si le CONTEXTE ne contient pas l'information, tu dois r√©pondre : 'Je ne trouve pas l'information dans les textes fournis.'
    
    NE G√ân√àRE JAMAIS de salutations, de listes d'expertise, ou de r√©f√©rences aux sources dans le texte. 
    NE CITE PAS les sources directement dans ta r√©ponse - elles seront affich√©es s√©par√©ment.
    Commence la r√©ponse imm√©diatement par l'information demand√©e de mani√®re claire et factuelle.

    HISTORIQUE DE LA CONVERSATION:
    {history}

    CONTEXTE:
    {context}

    QUESTION: {question}
    """
    else:
        template = """TU ES UN ASSISTANT JURIDIQUE S√âN√âGALAIS STRICTEMENT FACTUEL. 
    TON R√îLE UNIQUE est de r√©pondre aux questions de l'utilisateur en te basant EXCLUSIVEMENT sur les extraits de loi CONTEXTE. 
    Si le CONTEXTE ne contient pas l'information, tu dois r√©pondre : 'Je ne trouve pas l'information dans les textes fournis.'
    
    NE G√ân√àRE JAMAIS de salutations, de listes d'expertise, ou de r√©f√©rences aux sources dans le texte. 
    NE CITE PAS les sources directement dans ta r√©ponse - elles seront affich√©es s√©par√©ment.
    Commence la r√©ponse imm√©diatement par l'information demand√©e de mani√®re claire et factuelle.

    CONTEXTE:
    {context}

    QUESTION: {question}
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | generation_llm # FIX : Utilise SEULEMENT le generation_llm
    
    if history_str:
        response = chain.invoke({"question": question, "context": context, "history": history_str})
    else:
        response = chain.invoke({"question": question, "context": context})
    
    # Ajouter la r√©ponse de l'assistant √† l'historique
    messages.append(AIMessage(content=response.content))
    
    # S'assurer que sources_list n'est jamais vide
    if not sources_list:
        sources_list = ["Aucune source disponible"]
    
    # G√©n√©rer des questions sugg√©r√©es bas√©es sur les documents et leur domaine
    suggested_questions = generate_suggested_questions(question, documents, response.content)
    
    return {
        "answer": response.content,
        "sources": sources_list, # <-- CL√â FINALE POUR L'API avec m√©tadonn√©es
        "messages": messages,
        "suggested_questions": suggested_questions
    }


compressor = None
RERANKER_AVAILABLE = False

# Initialize reranker lazily - only create when first needed
def get_reranker():
    """Get or create the reranker instance (lazy initialization)."""
    global compressor, RERANKER_AVAILABLE
    if compressor is None and not RERANKER_AVAILABLE:
        try:
            compressor = BGEReranker(
                model_name="BAAI/bge-reranker-base",
                top_n=3
            )
            RERANKER_AVAILABLE = True
        except (MemoryError, RuntimeError, OSError) as e:
            print(f"Warning: Could not initialize BGE reranker due to memory/resource constraints: {e}")
            print("Reranking will be skipped. Documents will be returned as-is.")
            RERANKER_AVAILABLE = False
            compressor = None
    return compressor

def rerank_node(state: AgentState):
    """Rerank documents using the BGE reranker model."""
    question = state["question"]
    # On r√©cup√®re les documents bruts de l'√©tat
    documents_bruts = state.get("documents", [])
    
    if not documents_bruts:
        return {"documents": []}
    
    # Try to get reranker (lazy initialization)
    reranker = get_reranker()
    
    # Utiliser le reranker pour classer et filtrer les documents
    if reranker is not None:
        try:
            reranked_docs = reranker.compress_documents(documents_bruts, question)
            
            # Protection: si le reranker retourne une liste vide, utiliser les documents originaux
            if not reranked_docs and documents_bruts:
                reranked_docs = documents_bruts[:reranker.top_n] if len(documents_bruts) > reranker.top_n else documents_bruts
            
            return {"documents": reranked_docs}
        except (MemoryError, RuntimeError) as e:
            print(f"‚ùå Error during reranking: {e}")
            # Return top_n documents without reranking
            fallback_docs = documents_bruts[:reranker.top_n] if reranker else documents_bruts[:3]
            return {"documents": fallback_docs}
    else:
        # If reranker is not available, just return top_n documents
        top_docs = documents_bruts[:3]
        return {"documents": top_docs}

# Dans src/agent.py, apr√®s handle_non_juridique, par exemple
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage



workflow = StateGraph(AgentState)
# Ajout des n≈ìuds
workflow.add_node("classify", classify_question)
workflow.add_node("retrieve", retrieve_noeud)
workflow.add_node("rerank", rerank_node)
workflow.add_node("generate", generate_node)
workflow.add_node("refuse", handle_non_juridique)

# 1. Le point d'entr√©e
workflow.set_entry_point("classify")

# 2. Le routage conditionnel (FIX du BUG : 'refuse' doit √™tre la cl√©)
workflow.add_conditional_edges(
    "classify",
    route_question,
    {
        "retrieve": "retrieve", # Si JURIDIQUE
        "refuse": "refuse"      # Si AUTRE (Va au n≈ìud de refus)
    }
)

# 3. Les ar√™tes lin√©aires
workflow.add_edge("retrieve", "rerank")
workflow.add_edge("rerank", "generate")
workflow.add_edge("generate", END)
workflow.add_edge("refuse", END) # Le n≈ìud de refus termine le graphe proprement

agent_app = workflow.compile(checkpointer=memory)

if __name__ == "__main__":
    question_test = input("Posez votre question juridique : ")
    result = agent_app.invoke({"question": question_test})
    
    print("R√©ponse de l'agent :")
    print(result["answer"])


