from dotenv import load_dotenv
from typing import List, TypedDict, Optional
import os
import json
import random

from langchain_groq import ChatGroq
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# Custom BGE Reranker implementation
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import gc


class BGEReranker:
    """Custom reranker using BGE reranker model from HuggingFace with lazy loading."""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-base", top_n: int = 3, device: Optional[str] = None, enabled: bool = True):
        self.model_name = model_name
        self.top_n = top_n
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self._model = None
        self._tokenizer = None
        self.enabled = enabled  # Permet de d√©sactiver le reranker pour √©conomiser la m√©moire
    
    @property
    def tokenizer(self):
        """Lazy loading du tokenizer."""
        if self._tokenizer is None:
            try:
                self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors du chargement du tokenizer: {e}")
                raise
        return self._tokenizer
    
    @property
    def model(self):
        """Lazy loading du mod√®le avec optimisation m√©moire."""
        if self._model is None:
            try:
                print(f"üîÑ Chargement du mod√®le BGE Reranker ({self.model_name})...")
                # Utiliser torch_dtype=torch.float16 pour r√©duire la m√©moire de moiti√©
                # et low_cpu_mem_usage=True pour optimiser le chargement
                self._model = AutoModelForSequenceClassification.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device == "cpu" else torch.float32,
                    low_cpu_mem_usage=True,
                    device_map="auto" if self.device != "cpu" else None,
                )
                self._model.eval()
                self._model.to(self.device)
                # Forcer le garbage collection apr√®s le chargement
                gc.collect()
                if self.device == "cpu":
                    torch.set_num_threads(1)  # Limiter les threads CPU
                print(f"‚úÖ Mod√®le BGE Reranker charg√© sur {self.device}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors du chargement du mod√®le: {e}")
                # Si erreur, essayer sans sp√©cifier dtype
                try:
                    self._model = AutoModelForSequenceClassification.from_pretrained(
                        self.model_name,
                        low_cpu_mem_usage=True
                    )
                    self._model.eval()
                    self._model.to(self.device)
                    gc.collect()
                except Exception as e2:
                    print(f"‚ùå Erreur critique lors du chargement du mod√®le: {e2}")
                raise

        return self._model
    
    def compress_documents(
        self, documents: List[Document], query: str, batch_size: int = 8
    ) -> List[Document]:
        
        if not documents:
            return []
        
        # Si le reranker est d√©sactiv√©, retourner simplement les top_n documents
        if not self.enabled:
            return documents[:self.top_n]
        
        try:
            # Pr√©parer les paires (query, document)
            pairs = [[query, doc.page_content] for doc in documents]
            
            # Tokeniser et obtenir les scores
            tokenizer = self.tokenizer
            model = self.model
            
            # Traiter par batch pour √©viter les probl√®mes de m√©moire
            all_scores = []
            for i in range(0, len(pairs), batch_size):
                batch = pairs[i:i + batch_size]
                inputs = tokenizer(
                    batch,
                        padding=True,
                        truncation=True,
                        return_tensors="pt",
                        max_length=512
                    ).to(self.device)
                    
                with torch.no_grad():
                    scores = model(**inputs).logits.view(-1).float()
                    all_scores.extend(scores.cpu().tolist())
            
            # Cr√©er une liste de tuples (score, document) et trier
            scored_docs = list(zip(all_scores, documents))
            scored_docs.sort(reverse=True, key=lambda x: x[0])
            
            # Retourner les top_n documents
            return [doc for _, doc in scored_docs[:self.top_n]]
            
        except torch.cuda.OutOfMemoryError:
            print("‚ö†Ô∏è  M√©moire GPU insuffisante pour le reranking. Retour des documents originaux.")
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return documents[:self.top_n]
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors du reranking: {e}")
            # En cas d'erreur, retourner les documents originaux
            if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                # Return original documents if reranking fails due to memory
                return documents[:self.top_n]
            else:
                raise


load_dotenv()

# Utiliser un chemin absolu pour √©viter les probl√®mes de chemin relatif
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent
CHROMA_DB_PATH = BASE_DIR / "data" / "chroma_db"

# Configuration depuis les variables d'environnement
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEY n'est pas d√©finie dans les variables d'environnement")

# Option pour d√©sactiver le reranker (r√©duit l'utilisation m√©moire)
# Par d√©faut, d√©sactiv√© pour √©conomiser la m√©moire sur Render (plan starter 512MB)
ENABLE_RERANKER = os.getenv("ENABLE_RERANKER", "false").lower() == "true"

# Initialiser l'embedding function (lazy loading pour optimiser le d√©marrage)
_embedding_function = None
_db = None
_retriever = None

def get_embedding_function():
    """Lazy loading de l'embedding function avec optimisation m√©moire."""
    global _embedding_function
    if _embedding_function is None:
        _embedding_function = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={
                'device': 'cpu',
                'trust_remote_code': False,
            },
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 32,  # Traiter par batch pour optimiser la m√©moire
            }
        )
        # Forcer le garbage collection apr√®s le chargement
        gc.collect()
    return _embedding_function

def get_db():
    """Lazy loading de la base de donn√©es Chroma."""
    global _db
    if _db is None:
        if not CHROMA_DB_PATH.exists():
            raise FileNotFoundError(
                f"Base de donn√©es Chroma introuvable: {CHROMA_DB_PATH}\n"
                "Ex√©cutez: python src/ingestion.py"
            )
        
        _db = Chroma(
            persist_directory=str(CHROMA_DB_PATH),
            embedding_function=get_embedding_function(),
            collection_name="juridiction_senegal"
        )
        
        # V√©rifier que la base de donn√©es contient des documents
        try:
            collection = _db._collection  # type: ignore[attr-defined]
            count = collection.count() if collection else 0
            if count == 0:
                print("‚ö†Ô∏è  ATTENTION: La base de donn√©es Chroma existe mais ne contient aucun document.")
                print("   Ex√©cutez: python src/ingestion.py pour charger les documents.")

        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la v√©rification de la base de donn√©es: {e}")
    
    return _db

def get_retriever():
    """Lazy loading du retriever."""
    global _retriever
    if _retriever is None:
        db = get_db()
        _retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # Limiter √† 5 documents pour de meilleures performances
        )
    return _retriever

# Initialiser au d√©marrage (peut √™tre comment√© pour un vrai lazy loading)
try:
    db = get_db()
    retriever = get_retriever()
except Exception as e:
    print(f"‚ö†Ô∏è  Erreur lors de l'initialisation de la base de donn√©es: {e}")
    db = None
    retriever = None

# Configuration des LLMs avec gestion d'erreur
try:
    router_llm = ChatGroq(
        model_name="openai/gpt-oss-120b",
        temperature=0,
        max_tokens=50,  # Limiter pour la classification
        timeout=30,  # Timeout de 30 secondes
    )
    generation_llm = ChatGroq(
        model_name="openai/gpt-oss-120b",
        temperature=0,
        max_tokens=2000,  # Limiter la longueur des r√©ponses
        timeout=60,  # Timeout de 60 secondes
    )
except Exception as e:
    print(f"‚ùå Erreur lors de l'initialisation des LLMs: {e}")
    raise

# Initialiser le checkpointer pour la m√©moire des conversations
memory = MemorySaver()

class AgentState(TypedDict):
    question: str
    documents: List[Document]
    category: str
    answer: str
    sources: List[str]
    messages: List  # Historique des messages pour le checkpointer
    
    
    
def classify_question(state: AgentState):
    """N≈ìud qui utilise le router_llm pour classer la question (le plus rapide)."""
    # Ajouter le message de l'utilisateur √† l'historique
    messages = state.get("messages", [])
    messages.append(HumanMessage(content=state["question"]))
    
    question = state["question"].lower()
    
    # Liste de mots-cl√©s juridiques pour une classification rapide
    juridique_keywords = [
        "travail", "travailleur", "employeur", "employ√©", "salari√©", "contrat", 
        "licenciement", "pr√©avis", "retraite", "syndicat", "gr√®ve", "cong√©", 
        "salaire", "code du travail", "l.2", "l.69", "article l.",
        "p√©nal", "penal", "peine", "infraction", "sanction", "prison", "d√©tenu", 
        "juge", "tribunal", "proc√©dure", "prescription", "loi 2020", "code p√©nal",
        "constitution", "pr√©sident", "parlement", "pouvoir", "droit fondamental",
        "budget", "finance", "imp√¥t", "taxe", "fiscal", "d√©ficit", "ressource", 
        "charge", "plf", "loi de finance",
        "collectivit√©", "municipalit√©", "commune", "r√©gion",
        "aviation", "a√©rien",
        "droit", "loi", "d√©cret", "r√®glement", "juridique", "juridiction",
        "s√©n√©gal", "s√©n√©galais", "s√©n√©galaise"
    ]
    
    # Classification rapide bas√©e sur les mots-cl√©s (plus fiable)
    contains_juridique_keyword = any(keyword in question for keyword in juridique_keywords)
    
    # Si aucun mot-cl√© juridique n'est trouv√©, utiliser le LLM pour une classification plus fine
    if not contains_juridique_keyword:
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Tu es un classificateur binaire pour un assistant juridique s√©n√©galais.
Ta t√¢che est de d√©terminer si la question concerne le droit s√©n√©galais ou un sujet juridique g√©n√©ral.

Une question est JURIDIQUE si elle concerne :
- Le droit du travail (contrats, licenciement, cong√©s, salaires, retraite, etc.)
- Le droit p√©nal (infractions, peines, proc√©dures, tribunaux, etc.)
- Le droit constitutionnel (Constitution, pouvoirs, droits fondamentaux, etc.)
- Le droit financier (budget, imp√¥ts, finances publiques, etc.)
- Le droit administratif (collectivit√©s, organisation administrative, etc.)
- Toute question sur les lois, d√©crets, codes, r√®glements s√©n√©galais
- Toute question juridique g√©n√©rale m√™me sans mention explicite du S√©n√©gal

Une question est AUTRE si elle concerne :
- La m√©t√©o, le sport, la cuisine, les loisirs
- Des questions techniques non juridiques (programmation, math√©matiques pures, etc.)
- Des questions personnelles sans lien juridique

R√©ponds UNIQUEMENT avec le mot 'JURIDIQUE' ou 'AUTRE', sans autre texte."""),
            ("human", "{question}")
        ])
        
        try:
            chain = prompt | router_llm
            response = chain.invoke({"question": state["question"]})
            response_content = response.content.upper().strip()
            
            # Log pour le d√©bogage
            print(f"üîç Classification - Question: {state['question'][:50]}...")
            print(f"üîç R√©ponse du LLM: {response.content}")
            
            # D√©tection plus robuste de "JURIDIQUE"
            if "JURIDIQUE" in response_content or response_content.startswith("JURIDIQUE"):
                category = "JURIDIQUE"
            else:
                category = "AUTRE"
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la classification LLM: {e}")
            # En cas d'erreur, √™tre permissif et classer comme JURIDIQUE par d√©faut
            category = "JURIDIQUE"
    else:
        # Si des mots-cl√©s juridiques sont trouv√©s, classer directement comme JURIDIQUE
        category = "JURIDIQUE"
        print(f"‚úÖ Classification rapide - Question juridique d√©tect√©e par mots-cl√©s")
    
    print(f"üìä Cat√©gorie finale: {category}")
    
    return {"category": category, "messages": messages}

def handle_non_juridique(state: AgentState):
    """G√©n√®re une r√©ponse polie avec le router_llm lorsque la question est hors-sujet."""
    messages = state.get("messages", [])
    prompt = ChatPromptTemplate.from_template(
        "Tu es un assistant juridique s√©n√©galais. L'utilisateur a pos√© une question qui ne concerne pas le droit s√©n√©galais. R√©ponds poliment que tu ne peux r√©pondre qu'aux questions sur le droit s√©n√©galais (Constitution, Code du Travail, Code P√©nal, etc.). Sois bref et courtois."
    )
    chain = prompt | router_llm # FIX : Utilise SEULEMENT le router_llm (pour la vitesse)
    response = chain.invoke({"question": state["question"]})
    messages.append(AIMessage(content=response.content))
    
    return {
        "answer": response.content,
        # CHANGEMENT : on utilise la cl√© 'sources'
        "sources": ["Question jug√©e hors du champ d'expertise juridique."],
        "messages": messages,
        "suggested_questions": []
    }

def detect_domain_from_source(source_path: str) -> str:
    """D√©tecte le domaine juridique √† partir du chemin de la source."""
    source_lower = source_path.lower()
    
    # D√©tection bas√©e sur les chemins de fichiers et URLs
    if 'droitsocial' in source_lower or 'codedutravail' in source_lower or 'travail' in source_lower or 'retraite' in source_lower:
        return 'travail'
    elif 'droitpenal' in source_lower or 'penal' in source_lower or 'prescription' in source_lower:
        return 'penal'
    elif 'finance' in source_lower or 'budget' in source_lower or 'loi de finances' in source_lower:
        return 'finance'
    elif 'organisationadministration' in source_lower or 'fonction publique' in source_lower or 'administration' in source_lower:
        return 'administration'
    elif 'constitution' in source_lower or 'conseilconstitutionnel' in source_lower:
        return 'constitution'
    elif 'collectivites' in source_lower or 'collectivit√©s' in source_lower:
        return 'collectivites'
    elif 'aviation' in source_lower:
        return 'aviation'
    else:
        return 'general'

# Liste officielle et exhaustive des questions autoris√©es (issues des documents fournis)
AUTHORIZED_QUESTIONS = [
    "Quelles sont les missions du juge de l'application des peines au S√©n√©gal ?",
    "Comment fonctionne la commission p√©nitentiaire consultative de l'am√©nagement des peines ?",
    "Quelles sont les r√®gles de s√©paration des d√©tenus dans les √©tablissements p√©nitentiaires ?",
    "Quelles sont les conditions d'application du travail d'int√©r√™t g√©n√©ral ?",
    "Comment se d√©roule l'extraction d'un d√©tenu pour comparution devant un juge ?",
    "Quels sont les droits des d√©tenus provisoires selon le d√©cret 2001-362 ?",
    "Quel est le r√¥le des visiteurs de prison dans le syst√®me p√©nitentiaire ?",
    "Comment la loi 2020-05 modifie-t-elle les peines pour viol au S√©n√©gal ?",
    "Quelles sont les nouvelles peines pr√©vues pour les actes de p√©dophilie ?",
    "Quelles sont les circonstances aggravantes en mati√®re de violences sexuelles ?",
    "Quels d√©lais de prescription ont √©t√© suspendus pendant l'√©tat d'urgence ?",
    "Comment la loi 2020-16 affecte-t-elle les d√©lais de recours en mati√®re p√©nale ?",
    "Quelles sont les r√®gles concernant les contraintes par corps durant la p√©riode Covid-19 ?",
    "Quels dossiers sont jug√©s par les tribunaux d√©partementaux en mati√®re correctionnelle ?",
    "Quelles sont les infractions relevant uniquement du tribunal r√©gional ?",
    "Comment s'effectue le transfert d'une proc√©dure entre le tribunal r√©gional et le tribunal d√©partemental ?",
    "Qui est consid√©r√© comme travailleur selon l'article L.2 du Code du Travail ?",
    "Quelles sont les obligations de l'employeur envers les travailleurs ?",
    "Quelles sont les r√®gles de cr√©ation d'un syndicat professionnel ?",
    "Quelles protections s'appliquent aux travailleurs dans l'exercice du droit d'expression ?",
    "Quelles sont les infractions concernant le travail forc√© ?",
    "Quels sont les droits des syndicats devant la justice ?",
    "Comment fonctionne la proc√©dure de d√©p√¥t des statuts d'un syndicat ?",
    "Quelles sont les conditions d'acc√®s aux fonctions de direction syndicale ?",
    "Quelles protections s'appliquent aux biens d'un syndicat ?",
    "Quel est l'√¢ge l√©gal de d√©part √† la retraite au S√©n√©gal ?",
    "Quels travailleurs peuvent poursuivre leur activit√© au-del√† de 60 ans ?",
    "Quelles professions sont autoris√©es √† travailler jusqu'√† 65 ans ?",
    "Comment s'applique l'article L.69 modifi√© du Code du Travail ?",
    "Un travailleur peut-il continuer d'exercer volontairement apr√®s 60 ans ?",
    "Quels sont les axes strat√©giques du budget 2025 ?",
    "Comment se r√©partissent les ressources et charges de l'√âtat pour 2025 ?",
    "Quels sont les objectifs macro√©conomiques du PLF 2026 ?",
    "Quelles taxes nouvelles sont pr√©vues dans la strat√©gie SUPREC ?",
    "Quelles sont les mesures d'assainissement des finances publiques en 2026 ?",
    "Comment √©volue le d√©ficit budg√©taire entre 2024, 2025 et 2026 ?",
    "Quels sont les domaines de d√©penses prioritaires dans le budget 2026 ?",
    "Quels textes r√©gissent l'organisation p√©nitentiaire au S√©n√©gal ?",
    "Comment contester une d√©cision judiciaire en mati√®re correctionnelle ?",
    "Quelles sont les obligations de l'√âtat envers les travailleurs ?",
    "Comment d√©terminer l'autorit√© comp√©tente pour une infraction ?",
    "Quelles sont les r√®gles applicables aux syndicats ?",
    "Quelles sont les r√©centes r√©formes impactant le droit p√©nal s√©n√©galais ?",
    "Comment fonctionne la proc√©dure d'am√©nagement de peine ?",
    "Quel est le r√¥le de l'√âtat dans la protection sociale selon les budgets 2025/2026 ?",
]


def generate_suggested_questions(question: str, documents: List[Document], answer: str, conversation_history: Optional[str] = None) -> List[str]:
    """
    G√©n√®re exactement 3 questions sugg√©r√©es contextuelles en s√©lectionnant les questions les plus pertinentes
    parmi la liste officielle autoris√©e, bas√©es sur le contexte de la conversation.
    
    R√®gles:
    - Retourne exactement 3 questions les plus pertinentes selon le contexte
    - Utilise la question pos√©e, la r√©ponse donn√©e, les documents et l'historique pour d√©terminer la pertinence
    - Priorise les questions avec un score de pertinence √©lev√© (>= 2)
    - Compl√®te avec des questions du m√™me domaine si n√©cessaire
    - Ne retourne jamais de questions absentes de la liste
    - Ne retourne rien si pas de documents ou r√©ponse vide
    """
    # Si pas de documents ou r√©ponse vide, ne pas proposer de questions
    if not documents or not answer:
        return []
    
    # Ne pas proposer de questions si la r√©ponse est "Je ne trouve pas" ET qu'il n'y a vraiment pas de sources
    answer_stripped = answer.strip()
    if answer_stripped == "Je ne trouve pas l'information dans les textes fournis.":
        return []
    
    # Extraire les mots-cl√©s du contexte (question + r√©ponse + documents + historique)
    question_lower = question.lower()
    answer_lower = answer.lower()
    
    # Extraire les mots-cl√©s des documents
    doc_keywords = set()
    for doc in documents[:3]:  # Utiliser les 3 premiers documents
        if doc.page_content:
            # Extraire les mots significatifs (plus de 4 caract√®res)
            words = doc.page_content.lower().split()
            doc_keywords.update([w for w in words if len(w) > 4])
    
    # Extraire les mots-cl√©s de l'historique de conversation si disponible
    history_keywords = set()
    if conversation_history:
        history_words = conversation_history.lower().split()
        history_keywords.update([w for w in history_words if len(w) > 4])
    
    # Combiner tous les mots-cl√©s du contexte
    context_keywords = set(question_lower.split())
    context_keywords.update(answer_lower.split())
    context_keywords.update(doc_keywords)
    context_keywords.update(history_keywords)
    
    # D√©tecter le domaine principal de la conversation
    domain_keywords = {
        'travail': ['travail', 'travailleur', 'employeur', 'employ√©', 'salari√©', 'contrat', 'licenciement', 'pr√©avis', 'retraite', 'syndicat', 'gr√®ve', 'cong√©', 'salaire'],
        'penal': ['p√©nal', 'penal', 'peine', 'infraction', 'sanction', 'prison', 'd√©tenu', 'juge', 'tribunal', 'proc√©dure', 'prescription'],
        'finance': ['budget', 'finance', 'imp√¥t', 'taxe', 'fiscal', 'd√©ficit', 'ressource', 'charge'],
        'constitution': ['constitution', 'pr√©sident', 'parlement', 'pouvoir', 'droit fondamental'],
        'administration': ['administration', 'fonction publique', 'collectivit√©', 'organisation'],
    }
    
    detected_domain = 'general'
    max_matches = 0
    for domain, keywords in domain_keywords.items():
        matches = sum(1 for kw in keywords if kw in context_keywords)
        if matches > max_matches:
            max_matches = matches
            detected_domain = domain
    
    # Scorer chaque question selon sa pertinence au contexte
    question_scores = []
    for q in AUTHORIZED_QUESTIONS:
        score = 0
        q_lower = q.lower()
        
        # Score bas√© sur les mots-cl√©s communs avec la question
        question_words = set(q_lower.split())
        common_words = context_keywords.intersection(question_words)
        score += len(common_words) * 2  # Poids plus √©lev√© pour les mots communs
        
        # Score bas√© sur le domaine d√©tect√©
        if detected_domain == 'travail':
            if any(word in q_lower for word in ['travail', 'travailleur', 'employeur', 'employ√©', 'salari√©', 'contrat', 'licenciement', 'pr√©avis', 'retraite', 'syndicat', 'gr√®ve', 'cong√©', 'salaire', 'l.2', 'l.69']):
                score += 5
        elif detected_domain == 'penal':
            if any(word in q_lower for word in ['p√©nal', 'penal', 'peine', 'infraction', 'sanction', 'prison', 'd√©tenu', 'juge', 'tribunal', 'proc√©dure', 'prescription', 'loi 2020']):
                score += 5
        elif detected_domain == 'finance':
            if any(word in q_lower for word in ['budget', 'finance', 'imp√¥t', 'taxe', 'fiscal', 'd√©ficit', 'ressource', 'charge', '2025', '2026']):
                score += 5
        elif detected_domain == 'constitution':
            if any(word in q_lower for word in ['constitution', 'pr√©sident', 'parlement', 'pouvoir', 'droit fondamental']):
                score += 5
        
        # Score bas√© sur la similarit√© s√©mantique avec la question pos√©e
        # Si la question sugg√©r√©e contient des mots similaires √† la question pos√©e
        question_important_words = [w for w in question_lower.split() if len(w) > 4]
        q_important_words = [w for w in q_lower.split() if len(w) > 4]
        semantic_matches = len(set(question_important_words).intersection(set(q_important_words)))
        score += semantic_matches * 3
        
        # Bonus pour les questions sur le m√™me sujet mais avec un angle diff√©rent
        # (√©viter de sugg√©rer la m√™me question)
        if q_lower != question_lower:
            question_scores.append((score, q))
    
    # Trier par score d√©croissant et s√©lectionner les meilleures
    question_scores.sort(reverse=True, key=lambda x: x[0])
    
    # S√©lectionner exactement 3 questions les plus pertinentes
    num_questions = 3
    
    if len(question_scores) >= num_questions:
        # Prendre les 3 meilleures questions avec les scores les plus √©lev√©s
        # Si plusieurs questions ont le m√™me score, on peut en prendre al√©atoirement parmi celles-ci
        # Mais on privil√©gie toujours les scores les plus √©lev√©s
        top_questions = question_scores[:num_questions * 2]  # Prendre 2x plus pour avoir du choix si scores √©gaux
        
        # Grouper par score et prendre les meilleures
        selected = []
        for score, q in top_questions:
            if len(selected) >= num_questions:
                break
            # Si le score est significatif (au moins 2 points), l'inclure
            if score >= 2:
                selected.append(q)
            elif len(selected) < num_questions and score > 0:
                # Si on n'a pas encore 3 questions et que le score est > 0, l'inclure
                selected.append(q)
    else:
        # Si pas assez de questions avec score, prendre toutes celles disponibles
        selected = [q for _, q in question_scores[:num_questions]]
    
    # Si on n'a pas assez de questions pertinentes (score > 0), compl√©ter avec des questions du m√™me domaine
    if len(selected) < num_questions:
        # Essayer de trouver des questions du m√™me domaine
        domain_questions = []
        for q in AUTHORIZED_QUESTIONS:
            if q not in selected:
                q_lower = q.lower()
                if detected_domain == 'travail' and any(word in q_lower for word in ['travail', 'travailleur', 'employeur', 'employ√©', 'salari√©', 'contrat', 'licenciement', 'pr√©avis', 'retraite', 'syndicat']):
                    domain_questions.append(q)
                elif detected_domain == 'penal' and any(word in q_lower for word in ['p√©nal', 'penal', 'peine', 'infraction', 'sanction', 'prison', 'd√©tenu', 'juge', 'tribunal']):
                    domain_questions.append(q)
                elif detected_domain == 'finance' and any(word in q_lower for word in ['budget', 'finance', 'imp√¥t', 'taxe', 'fiscal', 'd√©ficit']):
                    domain_questions.append(q)
        
        # Ajouter des questions du m√™me domaine si disponibles
        if domain_questions:
            random.shuffle(domain_questions)
            selected.extend(domain_questions[:num_questions - len(selected)])
        
        # Si toujours pas assez, compl√©ter avec des questions al√©atoires
        if len(selected) < num_questions:
            remaining = [q for q in AUTHORIZED_QUESTIONS if q not in selected]
            random.shuffle(remaining)
            selected.extend(remaining[:num_questions - len(selected)])
    
    # Retourner exactement 3 questions
    return selected[:num_questions]


def retrieve_noeud(state: AgentState):
    question = state["question"]
    # Use the Chroma retriever to fetch relevant documents for the question
    try:
        documents = retriever.invoke(question)
        return {"documents": documents}
    except Exception as e:
        print(f"‚ùå ERREUR dans retrieve_noeud: {e}")
        return {"documents": []}

def generate_node(state: AgentState):
    """G√©n√®re la r√©ponse finale en utilisant le mod√®le de g√©n√©ration."""
    question = state["question"]
    documents = state.get("documents", [])
    messages = state.get("messages", [])
    
    # Construire l'historique de conversation √† partir des messages pr√©c√©dents
    history_str = ""
    if len(messages) > 1:  # Plus qu'un seul message (la question actuelle)
        # Prendre les 5 derniers √©changes (10 messages max)
        recent_messages = messages[-10:]
        history_parts = []
        for msg in recent_messages:
            if isinstance(msg, HumanMessage):
                history_parts.append(f"Utilisateur: {msg.content}")
            elif isinstance(msg, AIMessage):
                history_parts.append(f"Assistant: {msg.content}")
        if history_parts:
            history_str = "\n".join(history_parts)
    
    # Pr√©parer le contexte (sans r√©f√©rences inline) et les sources pour l'affichage
    context_parts = []
    sources_list = []
    
    if not documents:
        return {
            "answer": "Je ne trouve pas l'information dans les textes fournis.",
            "sources": ["Aucun document trouv√© pour cette question."],
            "messages": messages,
            "suggested_questions": []
        }
    
    # Extraire les mots-cl√©s de la question pour trouver les parties pertinentes
    question_words = set(question.lower().split())
    
    for idx, doc in enumerate(documents):
        # Extraire les m√©tadonn√©es
        metadata = doc.metadata if hasattr(doc, 'metadata') and doc.metadata else {}
        source = metadata.get('source', metadata.get('file_path', 'Document juridique'))
        page = metadata.get('page', metadata.get('page_number', None))
        
        # D√©tecter le domaine
        domain = detect_domain_from_source(str(source))
        
        # Extraire l'URL si pr√©sente
        url = None
        source_name = "Document juridique"
        
        if isinstance(source, str):
            # Si c'est une URL, l'extraire
            if source.startswith('http://') or source.startswith('https://'):
                url = source
                # Extraire un nom de document depuis l'URL
                if 'conseilconstitutionnel' in source.lower():
                    source_name = "Constitution du S√©n√©gal"
                elif 'primature' in source.lower():
                    if 'collectivites' in source.lower():
                        source_name = "Code des Collectivit√©s Locales"
                    elif 'aviation' in source.lower():
                        source_name = "Code de l'Aviation Civile"
                    else:
                        source_name = "Document Officiel"
                else:
                    source_name = os.path.basename(source)
            else:
                # C'est un chemin de fichier
                source_name = os.path.basename(source) if os.path.sep in source else source
                # Enlever l'extension si pr√©sente
                source_name = os.path.splitext(source_name)[0]
                # Nettoyer le nom (enlever les underscores, remplacer par espaces)
                source_name = source_name.replace('_', ' ').replace('-', ' ').title()
        else:
            source_name = str(source)
        
        # Ajouter le contenu au contexte (sans r√©f√©rence inline)
        if doc.page_content:
            context_parts.append(doc.page_content)
        
        # Extraire un extrait pertinent du contenu
        content = doc.page_content if doc.page_content else "(Contenu vide)"
        
        # Si le contenu est long, essayer de trouver la partie la plus pertinente
        if len(content) > 500:
            # Chercher les phrases contenant des mots-cl√©s de la question
            sentences = content.split('.')
            relevant_sentences = []
            
            for sentence in sentences:
                sentence_lower = sentence.lower()
                # Compter combien de mots-cl√©s sont pr√©sents dans la phrase
                matches = sum(1 for word in question_words if word in sentence_lower and len(word) > 3)
                if matches > 0:
                    relevant_sentences.append((matches, sentence))
            
            if relevant_sentences:
                # Trier par pertinence et prendre les meilleures phrases
                relevant_sentences.sort(reverse=True, key=lambda x: x[0])
                # Prendre jusqu'√† 3-4 phrases les plus pertinentes
                selected_sentences = [s[1] for s in relevant_sentences[:4]]
                # Trouver leur position dans le texte original
                start_pos = content.find(selected_sentences[0])
                if start_pos > 0:
                    # Commencer un peu avant pour avoir du contexte
                    start_pos = max(0, start_pos - 100)
                end_pos = content.find(selected_sentences[-1]) + len(selected_sentences[-1])
                if end_pos < len(content):
                    # Finir un peu apr√®s pour avoir du contexte
                    end_pos = min(len(content), end_pos + 100)
                
                # Extraire l'extrait pertinent
                extracted_content = content[start_pos:end_pos].strip()
                # Ajouter "..." si n√©cessaire
                if start_pos > 0:
                    extracted_content = "..." + extracted_content
                if end_pos < len(content):
                    extracted_content = extracted_content + "..."
                
                content = extracted_content
            else:
                # Si pas de correspondance, prendre le d√©but (mais essayer de commencer par une phrase compl√®te)
                first_period = content.find('.')
                if first_period > 0 and first_period < 200:
                    content = content[:min(600, len(content))]
                else:
                    # Prendre les premiers 500 caract√®res
                    content = content[:500] + "..."
        
        # Formater la source en JSON pour un parsing facile c√¥t√© frontend
        source_data = {
            "id": f"source_{idx}",
            "title": source_name,
            "url": url,
            "content": content,
            "page": page,
            "domain": domain
        }
        sources_list.append(json.dumps(source_data))
    
    context = "\n\n".join(context_parts)
    
    # Construire le template avec l'historique si disponible
    if history_str:
        template = """TU ES UN ASSISTANT JURIDIQUE S√âN√âGALAIS STRICTEMENT FACTUEL ET D√âTAILL√â. 
    TON R√îLE est de r√©pondre aux questions de l'utilisateur en te basant EXCLUSIVEMENT sur les extraits de loi CONTEXTE.
    
    R√àGLES CRITIQUES POUR TA R√âPONSE :
    1. SOIS CONCIS ET CLAIR : Limite ta r√©ponse √† 3-4 paragraphes maximum. Va droit au but, √©vite les r√©p√©titions et les d√©tails superflus.
    2. STRUCTURE TA R√âPONSE AVEC HI√âRARCHIE :
       - Commence par une r√©ponse directe et concise (1-2 phrases)
       - Utilise des listes √† puces (-) pour les points importants, les missions, les conditions, etc.
       - Utilise des listes num√©rot√©es (1., 2., 3.) pour les √©tapes ou processus
       - Termine par les r√©f√©rences l√©gales entre crochets [Article X, Code Y]
    3. UTILISE DES LISTES POUR FACILITER LA LECTURE : Au lieu de longs paragraphes, utilise des listes √† puces pour les √©l√©ments multiples (missions, conditions, droits, obligations, etc.).
    4. SOIS UN VRAI ASSISTANT P√âDAGOGIQUE : Explique le droit de mani√®re simple et accessible, sans jargon inutile.
    5. INCLUS TOUJOURS les d√©tails sp√©cifiques : nombres, dates, montants, d√©lais, mais de mani√®re concise.
    6. NE COMMENCE JAMAIS par citer un article : Commence par la r√©ponse concr√®te.
    7. NE METS JAMAIS de titres ou sections : √âcris de mani√®re fluide mais structur√©e avec des listes.
    8. Si le CONTEXTE ne contient pas l'information, r√©ponds : 'Je ne trouve pas l'information dans les textes fournis.'
    
    EXEMPLES DE BONNES R√âPONSES :
    - Question: "Quel est l'√¢ge l√©gal de d√©part √† la retraite ?"
      Bonne r√©ponse: "Au S√©n√©gal, un travailleur peut prendre sa retraite √† partir de 60 ans. C'est l'√¢ge minimum fix√© par la loi pour pouvoir b√©n√©ficier de la retraite. Pour pouvoir partir √† la retraite, il faut g√©n√©ralement avoir atteint cet √¢ge ET avoir cotis√© pendant un certain nombre d'ann√©es (les conditions exactes d√©pendent du r√©gime de retraite). Cette r√®gle de 60 ans est pr√©vue par l'article L.69 du Code du Travail. [R√©f√©rence pour sp√©cialistes : Article L.69 du Code du Travail]"
      Mauvaise r√©ponse: "R√©ponse directe et simple : 60 ans. Explication d√©taill√©e : L'article L.69..." (ne pas mettre de titres)
      Mauvaise r√©ponse: "Selon l'article L.69 du Code du Travail, l'√¢ge de la retraite est de 60 ans." (trop technique, commence par l'article)
    
    - Question: "Quelle est la dur√©e du pr√©avis ?"
      Bonne r√©ponse: "Le pr√©avis est la p√©riode pendant laquelle vous continuez de travailler apr√®s avoir √©t√© inform√© de la fin de votre contrat. Cette p√©riode vous permet de vous pr√©parer √† la fin de votre emploi. Au S√©n√©gal, la dur√©e du pr√©avis d√©pend de votre anciennet√© dans l'entreprise : si vous travaillez depuis moins de 2 ans, le pr√©avis est de 1 mois. Si vous travaillez entre 2 et 5 ans, il est de 2 mois. Et si vous travaillez depuis plus de 5 ans, il est de 3 mois. Cette r√®gle prot√®ge les travailleurs en leur donnant le temps de trouver un nouvel emploi. [R√©f√©rences : Code du Travail, articles relatifs au pr√©avis]"
      Mauvaise r√©ponse: "R√©ponse directe : Le pr√©avis varie. Explication d√©taill√©e : Selon l'anciennet√©..." (ne pas mettre de titres)
      Mauvaise r√©ponse: "Le pr√©avis est pr√©vu par le Code du Travail selon l'anciennet√©." (trop vague, ne donne pas les dur√©es)
    
    NE G√ân√àRE JAMAIS de salutations, de listes d'expertise, ou de r√©f√©rences aux sources dans le texte. 
    NE CITE PAS les sources directement dans ta r√©ponse - elles seront affich√©es s√©par√©ment.
    Commence la r√©ponse imm√©diatement par l'information demand√©e de mani√®re claire, factuelle et D√âTAILL√âE.

    HISTORIQUE DE LA CONVERSATION:
    {history}

    CONTEXTE:
    {context}

    QUESTION: {question}
    
    R√âPONSE (pr√©cise, d√©taill√©e, avec tous les chiffres et d√©tails du contexte):
    """
    else:
        template = """TU ES UN ASSISTANT JURIDIQUE S√âN√âGALAIS STRICTEMENT FACTUEL ET D√âTAILL√â. 
    TON R√îLE est de r√©pondre aux questions de l'utilisateur en te basant EXCLUSIVEMENT sur les extraits de loi CONTEXTE.
    
    R√àGLES CRITIQUES POUR TA R√âPONSE :
    1. SOIS CONCIS ET CLAIR : Limite ta r√©ponse √† 3-4 paragraphes maximum. Va droit au but, √©vite les r√©p√©titions et les d√©tails superflus.
    2. STRUCTURE TA R√âPONSE AVEC HI√âRARCHIE :
       - Commence par une r√©ponse directe et concise (1-2 phrases)
       - Utilise des listes √† puces (-) pour les points importants, les missions, les conditions, etc.
       - Utilise des listes num√©rot√©es (1., 2., 3.) pour les √©tapes ou processus
       - Termine par les r√©f√©rences l√©gales entre crochets [Article X, Code Y]
    3. UTILISE DES LISTES POUR FACILITER LA LECTURE : Au lieu de longs paragraphes, utilise des listes √† puces pour les √©l√©ments multiples (missions, conditions, droits, obligations, etc.).
    4. SOIS UN VRAI ASSISTANT P√âDAGOGIQUE : Explique le droit de mani√®re simple et accessible, sans jargon inutile.
    5. INCLUS TOUJOURS les d√©tails sp√©cifiques : nombres, dates, montants, d√©lais, mais de mani√®re concise.
    6. NE COMMENCE JAMAIS par citer un article : Commence par la r√©ponse concr√®te.
    7. NE METS JAMAIS de titres ou sections : √âcris de mani√®re fluide mais structur√©e avec des listes.
    8. Si le CONTEXTE ne contient pas l'information, r√©ponds : 'Je ne trouve pas l'information dans les textes fournis.'
    
    EXEMPLES DE BONNES R√âPONSES :
    - Question: "Quel est l'√¢ge l√©gal de d√©part √† la retraite ?"
      Bonne r√©ponse: "Au S√©n√©gal, un travailleur peut prendre sa retraite √† partir de 60 ans. C'est l'√¢ge minimum fix√© par la loi pour pouvoir b√©n√©ficier de la retraite. Pour pouvoir partir √† la retraite, il faut g√©n√©ralement avoir atteint cet √¢ge ET avoir cotis√© pendant un certain nombre d'ann√©es (les conditions exactes d√©pendent du r√©gime de retraite). Cette r√®gle de 60 ans est pr√©vue par l'article L.69 du Code du Travail. [R√©f√©rence pour sp√©cialistes : Article L.69 du Code du Travail]"
      Mauvaise r√©ponse: "R√©ponse directe et simple : 60 ans. Explication d√©taill√©e : L'article L.69..." (ne pas mettre de titres)
      Mauvaise r√©ponse: "Selon l'article L.69 du Code du Travail, l'√¢ge de la retraite est de 60 ans." (trop technique, commence par l'article)
    
    - Question: "Quelle est la dur√©e du pr√©avis ?"
      Bonne r√©ponse: "Le pr√©avis est la p√©riode pendant laquelle vous continuez de travailler apr√®s avoir √©t√© inform√© de la fin de votre contrat. Cette p√©riode vous permet de vous pr√©parer √† la fin de votre emploi. Au S√©n√©gal, la dur√©e du pr√©avis d√©pend de votre anciennet√© dans l'entreprise : si vous travaillez depuis moins de 2 ans, le pr√©avis est de 1 mois. Si vous travaillez entre 2 et 5 ans, il est de 2 mois. Et si vous travaillez depuis plus de 5 ans, il est de 3 mois. Cette r√®gle prot√®ge les travailleurs en leur donnant le temps de trouver un nouvel emploi. [R√©f√©rences : Code du Travail, articles relatifs au pr√©avis]"
      Mauvaise r√©ponse: "R√©ponse directe : Le pr√©avis varie. Explication d√©taill√©e : Selon l'anciennet√©..." (ne pas mettre de titres)
      Mauvaise r√©ponse: "Le pr√©avis est pr√©vu par le Code du Travail selon l'anciennet√©." (trop vague, ne donne pas les dur√©es)
    
    NE G√ân√àRE JAMAIS de salutations, de listes d'expertise, ou de r√©f√©rences aux sources dans le texte. 
    NE CITE PAS les sources directement dans ta r√©ponse - elles seront affich√©es s√©par√©ment.
    Commence la r√©ponse imm√©diatement par l'information demand√©e de mani√®re claire, factuelle et D√âTAILL√âE.

    CONTEXTE:
    {context}

    QUESTION: {question}
    
    R√âPONSE (pr√©cise, d√©taill√©e, avec tous les chiffres et d√©tails du contexte):
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | generation_llm # FIX : Utilise SEULEMENT le generation_llm
    
    if history_str:
        response = chain.invoke({"question": question, "context": context, "history": history_str})
    else:
        response = chain.invoke({"question": question, "context": context})
    
    # Ajouter la r√©ponse de l'assistant √† l'historique
    messages.append(AIMessage(content=response.content))
    
    # CORRECTION : Si des sources ont √©t√© trouv√©es mais que le LLM r√©pond "Je ne trouve pas...",
    # c'est incoh√©rent. On ne doit jamais retourner "Je ne trouve pas" si des sources existent.
    answer_content = response.content.strip()
    
    # Si des sources existent mais que la r√©ponse dit "Je ne trouve pas", c'est incoh√©rent
    # Dans ce cas, on utilise le contexte des documents pour g√©n√©rer une r√©ponse
    if sources_list and len(sources_list) > 0 and answer_content == "Je ne trouve pas l'information dans les textes fournis.":
        # Si on a des sources, on ne devrait jamais dire qu'on ne trouve pas l'information
        # On va utiliser le contexte pour reformuler une r√©ponse
        if context and len(context.strip()) > 0:
            # Prendre les premiers 500 caract√®res du contexte comme base de r√©ponse
            context_excerpt = context[:500].strip()
            if len(context) > 500:
                context_excerpt += "..."
            # Reformuler avec le LLM en for√ßant une r√©ponse bas√©e sur le contexte
            reformulation_prompt = f"""Bas√© sur le contexte suivant, r√©ponds √† la question de mani√®re factuelle et concise.
Ne dis jamais "Je ne trouve pas" car le contexte contient des informations.

CONTEXTE:
{context_excerpt}

QUESTION: {question}

R√âPONSE (factuelle et bas√©e uniquement sur le contexte):"""
            
            try:
                reformulation_chain = ChatPromptTemplate.from_template(reformulation_prompt) | generation_llm
                reformulated_response = reformulation_chain.invoke({})
                answer_content = reformulated_response.content.strip()
                # Si la reformulation retourne encore "Je ne trouve pas", utiliser directement le contexte
                if answer_content == "Je ne trouve pas l'information dans les textes fournis.":
                    answer_content = f"D'apr√®s les documents juridiques consult√©s : {context_excerpt}"
            except Exception:
                # En cas d'erreur, utiliser directement un extrait du contexte
                answer_content = f"D'apr√®s les documents juridiques consult√©s : {context_excerpt}"
    
    # S'assurer que sources_list n'est jamais vide
    if not sources_list:
        sources_list = ["Aucune source disponible"]
    
    # G√©n√©rer des questions sugg√©r√©es bas√©es sur les documents et leur domaine
    # Ne pas g√©n√©rer de questions si la r√©ponse est "Je ne trouve pas" ET qu'il n'y a pas de sources
    if answer_content == "Je ne trouve pas l'information dans les textes fournis." and not sources_list:
        suggested_questions = []
    else:
        # G√©n√©rer des questions sugg√©r√©es contextuelles en incluant l'historique
        suggested_questions = generate_suggested_questions(
            question, 
            documents, 
            answer_content,
            conversation_history=history_str if history_str else None
        )
    
    return {
        "answer": answer_content,
        "sources": sources_list, # <-- CL√â FINALE POUR L'API avec m√©tadonn√©es
        "messages": messages,
        "suggested_questions": suggested_questions
    }


compressor = None

# Cr√©er le graphe d'agent
workflow = StateGraph(AgentState)

# Ajouter les n≈ìuds
workflow.add_node("classify", classify_question)
workflow.add_node("retrieve", retrieve_noeud)
workflow.add_node("generate", generate_node)
workflow.add_node("non_juridique", handle_non_juridique)

# D√©finir le point d'entr√©e
workflow.set_entry_point("classify")

# Ajouter les ar√™tes conditionnelles
def should_retrieve(state: AgentState):
    category = state.get("category", "")
    if category == "JURIDIQUE":
        return "retrieve"
    else:
        return "non_juridique"

workflow.add_conditional_edges("classify", should_retrieve)
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)
workflow.add_edge("non_juridique", END)

# Compiler le graphe avec le checkpointer pour la m√©moire
agent_app = workflow.compile(checkpointer=memory)
